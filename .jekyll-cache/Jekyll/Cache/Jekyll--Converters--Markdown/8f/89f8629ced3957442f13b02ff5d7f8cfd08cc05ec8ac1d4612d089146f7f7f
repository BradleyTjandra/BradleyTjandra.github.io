I"≈<p>There are my notes from reading the paper <em>Incorrigibility in CIRL</em>, and from the subsequent discussion.</p>

<blockquote>
  <p><a href="https://arxiv.org/pdf/1709.06275.pdf">Link to paper</a></p>
</blockquote>

<!-- MarkdownTOC -->

<ul>
  <li><a href="#introduction">Introduction</a></li>
  <li><a href="#the-paper">The paper</a>
    <ul>
      <li><a href="#utility-indifference">Utility Indifference</a></li>
    </ul>
  </li>
  <li><a href="#discussion">Discussion</a>
    <ul>
      <li><a href="#utility-indifference-1">Utility Indifference</a></li>
      <li><a href="#off-switch-as-information">Off-Switch as Information</a></li>
    </ul>
  </li>
</ul>

<!-- /MarkdownTOC -->

<h2 id="introduction">Introduction</h2>

<p><strong>CIRL</strong> is a type of machine learning problem where</p>

<ul>
  <li>The AI is trying to learn which actions will give it the highest reward (RL)</li>
  <li>The AI does not know what the reward function is and instead looks to the human‚Äôs actions to imply what the latter deems valauble (IRL)</li>
  <li>The AI does not assume the human acts optimally; the AI can imply the human‚Äôs reward function from other behaviours i.e. ‚Äúactive teaching, active learning and communicative actions‚Äù meant to best inform the AI what the human wants to achieve (CIRL)</li>
</ul>

<h2 id="the-paper">The paper</h2>

<p>However, this requires the AI to have some probability distribution of the human‚Äôs reward function. If this posterior is correct, then CIRL will lead to non-negative behaviour when the AI deviates from human-recommended actions. However, this is a big assumption! <em>Incorrigibility in CIRL</em> demonstrates that if the AI has an incorrect posterior, it may lead to sub-optimal behaviour.</p>

<p>The paper advocates for a system which</p>

<ol>
  <li>Has a simple, easy-to-verify module which automatically shutsdown when issued a shutdown command</li>
  <li>Has no incentive to remove, damage or ignore its shutdown module</li>
  <li>Has a small incentive to keep its shutdown module around</li>
</ol>

<p>Even under a ‚Äúbroad range‚Äù of incorrect reward functions, priors over the human reward parameters, which actions are available, ‚Äúetc.‚Äù.</p>

<p>To do this, Carey shows that:</p>

<ul>
  <li>The <strong>AI with an incorrect reward function posterior</strong> achieves <strong>sub-optimal results and ignore the human‚Äôs suggested action</strong>, because it thinks the latter is acting sub-optimally</li>
  <li>The same <strong>AI with a shutdown button</strong> may <strong>ignore a human‚Äôs instruction to shutdown</strong> for the same reason</li>
  <li>The same <strong>AI with a module which automatically shutsdown</strong> the AI when issued the shutdown command is <strong>optimal</strong></li>
  <li>But the same <strong>AI with the ability to remove this shutdown module</strong> will elect to do so, so long as the penalty is not too large</li>
  <li>An <strong>AI who is incentivised too heavily to shutdown</strong> will always do so, even if it‚Äôs not optimal for the human</li>
  <li>An <strong>AI with ‚Äúutility indifference‚Äù</strong> and a small penalty for removing its shutdown module will <strong>act optimally</strong></li>
  <li>But an <strong>AI with ‚Äúutility indifference‚Äù who can gain a small reward for removing its shutdown module</strong> will not. Carey points out this may more often be the case, where removing the shutdown module may allow for a performance boost because the physical or computational resources of the shutdown module could be used more effectively otherwise</li>
</ul>

<h3 id="utility-indifference">Utility Indifference</h3>

<p>This is a suggestion from Stuart Armstrong that the reward from shutting down should be equal to the expected reward of not shutting down.</p>

<p>This makes the AI <em>indifferent</em> to taking the shutdown command and not, and hence when the human suggests that it shutdown, it will be willing to.</p>

<p>There are two ways to determine this:</p>

<ol>
  <li>At the beginning of the episode, for every attainable, non-terminal state, calculate the expected reward of not shuttng down once you are at that state</li>
  <li>Dynamically calculate the expected reward of not shutting down</li>
</ol>

<p>The issue with the first is that is that the AI may still want to remove its shutdown module if it can gain a small performance boost as discussed in the previous section.</p>

<p>The second method means the AI will be indifferent between shutting down and removing its shutdown module. However, this continuous ‚Äúre-balancing‚Äù of the shutdown button‚Äôs reward makes the AI a kind of Buridan‚Äôs ass, forever asking the human whether it should shutdown in order to resolve tie breaks.</p>

<h2 id="discussion">Discussion</h2>

<p>At our AI Meetup, we discussed two points:</p>

<ol>
  <li>Using utility indifference to disincentivise removing the off-switch</li>
  <li>Using the off-switch as information which will indicate to the AI that it should turn off</li>
</ol>

<h3 id="utility-indifference-1">Utility Indifference</h3>

<p>In the first case, we proposed making the reward associated with removing the shutdown module equal to $-R -0.01$ where $R$ is the expected reward after the shutdown module is removed. In this case, the AI is disincentivised to remove its shutdown module, because any reward it could get is offset by the cost of removing the module.</p>

<p>Of course, it is not simply to know flag in advance all actions which would lead to the off-switch being removed. Obvious ones are things such as causing direct physical contact with the module.</p>

<h3 id="off-switch-as-information">Off-Switch as Information</h3>

<p>In the second case, we discussed whether an AI would always ignore a human‚Äôs instruction to shutdown in the second scenario (see the above bullet points). We agreed that, if the robot has some type of unbound distribution on its estimate of the reward and believes that human is rational $1-\epsilon$ of the time (for $\epsilon \neq 1$), then with sufficient shutdown commands from the human, it would update on its reward function.</p>

<p>We showed this held in a simple case of assuming there was Gaussian uncertainty around the AI‚Äôs reward estimate, even though the mean of the AI‚Äôs estimates was always positive (that is, it expects not shutting off will yield positive value). That is, after $n$ shutdown commands, the expected value of not shutting down is estimated at:</p>

\[\mathbb E(R | o_{1:n} = a_{SD}) = \mu - (1 - \epsilon^n)\sigma \frac{\phi(-\mu/\sigma)}{\Phi(-\mu/\sigma)}\]

<p>We see the automatic shutdown condition as a special case where $\epsilon=0$, where the above formula doesn‚Äôt hold.</p>

<p>We discussed further that the ideal case would be to paramterise the uncertainty around the reward and the prior on human irrationality $\epsilon$ so that the AI would avoid removing its ability to receive shutdown commands, as receiving these commands provides information about the reward. We did not get the opportunity to discuss this further.</p>
:ET